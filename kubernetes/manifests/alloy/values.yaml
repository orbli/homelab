# Grafana Alloy Configuration for ArgoCD deployment
# This collects logs, metrics, and traces from all pods

alloy:
  configMap:
    create: true
    content: |
      // ====== LOG COLLECTION ======
      // Discover Kubernetes pods for logs
      discovery.kubernetes "pods" {
        role = "pod"
      }
      
      // Relabel to add metadata for logs
      discovery.relabel "pods" {
        targets = discovery.kubernetes.pods.targets
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label  = "node"
        }
      }
      
      // Collect logs using Kubernetes API
      loki.source.kubernetes "pods" {
        targets    = discovery.relabel.pods.output
        forward_to = [loki.write.default.receiver]
      }
      
      // Write logs to Loki
      loki.write "default" {
        endpoint {
          url = "http://loki.observability.svc.home-hk1-cluster.orbb.li:3100/loki/api/v1/push"
        }
      }
      
      // ====== OTLP COLLECTION (TRACES, METRICS, LOGS) ======
      // OTLP receiver for traces, metrics, and logs
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        
        http {
          endpoint = "0.0.0.0:4318"
        }
        
        output {
          traces  = [otelcol.processor.batch.traces.input]
          metrics = [otelcol.processor.batch.metrics.input]
          logs    = [otelcol.processor.batch.logs.input]
        }
      }
      
      // ====== BATCH PROCESSORS ======
      // Batch processor for traces
      otelcol.processor.batch "traces" {
        output {
          traces = [otelcol.processor.k8sattributes.traces.input]
        }
      }
      
      // Batch processor for metrics
      otelcol.processor.batch "metrics" {
        output {
          metrics = [otelcol.processor.k8sattributes.metrics.input]
        }
      }
      
      // Batch processor for logs
      otelcol.processor.batch "logs" {
        output {
          logs = [otelcol.processor.k8sattributes.logs.input]
        }
      }
      
      // ====== K8S ATTRIBUTE PROCESSORS ======
      // Add Kubernetes attributes to traces
      otelcol.processor.k8sattributes "traces" {
        extract {
          metadata = ["k8s.namespace.name", "k8s.pod.name", "k8s.deployment.name", "k8s.node.name"]
        }
        
        output {
          traces = [otelcol.exporter.otlp.tempo.input]
        }
      }
      
      // Add Kubernetes attributes to metrics
      otelcol.processor.k8sattributes "metrics" {
        extract {
          metadata = ["k8s.namespace.name", "k8s.pod.name", "k8s.deployment.name", "k8s.node.name"]
        }
        
        output {
          metrics = [otelcol.exporter.prometheus.otlp_metrics.input]
        }
      }
      
      // Add Kubernetes attributes to logs
      otelcol.processor.k8sattributes "logs" {
        extract {
          metadata = ["k8s.namespace.name", "k8s.pod.name", "k8s.deployment.name", "k8s.node.name"]
        }
        
        output {
          logs = [otelcol.exporter.loki.otlp.input]
        }
      }
      
      // ====== EXPORTERS ======
      // Export traces to Tempo
      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo.observability.svc.home-hk1-cluster.orbb.li:4317"
          tls {
            insecure = true
          }
        }
      }
      
      // Convert and export OTLP metrics to Prometheus remote write
      otelcol.exporter.prometheus "otlp_metrics" {
        forward_to = [prometheus.remote_write.default.receiver]
        add_metric_suffixes = false
      }
      
      // Export OTLP logs to Loki
      otelcol.exporter.loki "otlp" {
        forward_to = [loki.write.default.receiver]
      }
      
      // ====== PROMETHEUS REMOTE WRITE ======
      // Send metrics to Prometheus via remote write
      prometheus.remote_write "default" {
        endpoint {
          url = "http://prometheus-kube-prometheus-prometheus.observability.svc.home-hk1-cluster.orbb.li:9090/api/v1/write"
          
          // Optional: Add metadata labels to identify OTLP metrics source
          // You can customize this based on your needs
          write_relabel_config {
            source_labels = ["__name__"]
            regex = ".*"
            target_label = "telemetry_source"
            replacement = "alloy_otlp"
          }
        }
      }
      
      // ====== PROFILE COLLECTION FOR PYROSCOPE ======
      // Use the same pod discovery from above for profiles
      
      // Filter pods with any profile annotations
      discovery.relabel "profile_pods" {
        targets = discovery.kubernetes.pods.targets
        
        // Keep pods with any profile annotation
        // We'll look for memory scrape as the primary indicator
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape"]
          regex = "true"
          action = "keep"
        }
        
        // Set the address using pod IP and port from annotation
        rule {
          source_labels = ["__meta_kubernetes_pod_ip", "__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port"]
          separator = ":"
          target_label = "__address__"
        }
        
        // Add metadata labels
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "node"
        }
      }
      
      // Single scraper for all profile types
      pyroscope.scrape "kubernetes_pods" {
        targets = discovery.relabel.profile_pods.output
        
        profiling_config {
          // Memory profiling (heap)
          profile.memory {
            enabled = true
            path = "/debug/pprof/heap"
          }
          // Goroutine profiling
          profile.goroutine {
            enabled = true
            path = "/debug/pprof/goroutine"
          }
          // Mutex contention profiling
          profile.mutex {
            enabled = true
            path = "/debug/pprof/mutex"
          }
          // Block profiling  
          profile.block {
            enabled = true
            path = "/debug/pprof/block"
          }
          // CPU profiling (this one actually blocks during collection)
          profile.process_cpu {
            enabled = true
            path = "/debug/pprof/profile"
          }
        }
        
        forward_to = [pyroscope.write.profiles.receiver]
      }
      
      // Write profiles to Pyroscope
      pyroscope.write "profiles" {
        endpoint {
          url = "http://pyroscope.observability.svc.home-hk1-cluster.orbb.li:4040"
        }
      }

  # Extra ports for OTLP receivers
  extraPorts:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

# RBAC
rbac:
  create: true

# Service Account
serviceAccount:
  create: true
  name: alloy

# Resources
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 512Mi