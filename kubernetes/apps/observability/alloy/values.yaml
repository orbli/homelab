# Grafana Alloy Configuration for ArgoCD deployment
# This collects logs, metrics, and traces from all pods

alloy:
  configMap:
    create: true
    content: |
      // ====== LOG COLLECTION ======
      // Discover Kubernetes pods for logs
      discovery.kubernetes "pods" {
        role = "pod"
      }
      
      // Relabel to add metadata for logs
      discovery.relabel "pods" {
        targets = discovery.kubernetes.pods.targets
        
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }
        
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label  = "node"
        }
      }
      
      // Collect logs using Kubernetes API
      loki.source.kubernetes "pods" {
        targets    = discovery.relabel.pods.output
        forward_to = [loki.write.default.receiver]
      }
      
      // Write logs to Loki
      loki.write "default" {
        endpoint {
          url = "http://loki.observability.svc.home-hk1-cluster.orbb.li:3100/loki/api/v1/push"
        }
      }
      
      // ====== TRACE COLLECTION (OTEL) ======
      // OTLP receiver for traces
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        
        http {
          endpoint = "0.0.0.0:4318"
        }
        
        output {
          traces  = [otelcol.processor.batch.default.input]
        }
      }
      
      // Batch processor for better performance
      otelcol.processor.batch "default" {
        output {
          traces = [otelcol.processor.k8sattributes.default.input]
        }
      }
      
      // Add Kubernetes attributes to traces
      otelcol.processor.k8sattributes "default" {
        extract {
          metadata = ["k8s.namespace.name", "k8s.pod.name", "k8s.deployment.name", "k8s.node.name"]
        }
        
        output {
          traces = [otelcol.exporter.otlp.tempo.input]
        }
      }
      
      // Export traces to Tempo Distributor
      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo-distributor.observability.svc.home-hk1-cluster.orbb.li:4317"
          tls {
            insecure = true
          }
        }
      }
      
      // ====== PROFILE COLLECTION FOR PYROSCOPE ======
      // Use the same pod discovery from above for profiles
      
      // Filter pods with any profile annotations
      discovery.relabel "profile_pods" {
        targets = discovery.kubernetes.pods.targets
        
        // Keep pods with any profile annotation
        // We'll look for memory scrape as the primary indicator
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_scrape"]
          regex = "true"
          action = "keep"
        }
        
        // Set the address using pod IP and port from annotation
        rule {
          source_labels = ["__meta_kubernetes_pod_ip", "__meta_kubernetes_pod_annotation_profiles_grafana_com_memory_port"]
          separator = ":"
          target_label = "__address__"
        }
        
        // Add metadata labels
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "node"
        }
      }
      
      // Single scraper for all profile types
      pyroscope.scrape "kubernetes_pods" {
        targets = discovery.relabel.profile_pods.output
        
        profiling_config {
          // Memory profiling (heap)
          profile.memory {
            enabled = true
            path = "/debug/pprof/heap"
          }
          // Goroutine profiling
          profile.goroutine {
            enabled = true
            path = "/debug/pprof/goroutine"
          }
          // Mutex contention profiling
          profile.mutex {
            enabled = true
            path = "/debug/pprof/mutex"
          }
          // Block profiling  
          profile.block {
            enabled = true
            path = "/debug/pprof/block"
          }
          // CPU profiling (this one actually blocks during collection)
          profile.process_cpu {
            enabled = true
            path = "/debug/pprof/profile"
          }
        }
        
        forward_to = [pyroscope.write.profiles.receiver]
      }
      
      // Write profiles to Pyroscope
      pyroscope.write "profiles" {
        endpoint {
          url = "http://pyroscope.observability.svc.home-hk1-cluster.orbb.li:4040"
        }
      }

# Deployment configuration
controller:
  type: deployment
  replicas: 1

# Service
service:
  enabled: true
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: TCP
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

# RBAC
rbac:
  create: true

# Service Account
serviceAccount:
  create: true
  name: alloy

# Resources
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 512Mi